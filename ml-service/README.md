# ðŸš€ ML Service - Python FastAPI Backend

## Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Start service
python app.py

# Or with uvicorn directly
uvicorn app:app --host 0.0.0.0 --port 8000 --reload
```

## API Endpoints

### Check URL

```bash
POST http://localhost:8000/api/predict
Content-Type: application/json

{
    "url": "https://example.com"
}
```

**Response**:

```json
{
  "url": "https://example.com",
  "is_phishing": false,
  "confidence": 0.123,
  "threat_level": "SAFE",
  "details": {
    "prediction": 0,
    "feature_extraction_ms": 45.2,
    "ml_inference_ms": 2.1,
    "models_used": ["lightgbm", "xgboost"]
  },
  "latency_ms": 52.3,
  "model_version": "1.0.0"
}
```

### Health Check

```bash
GET http://localhost:8000/health
```

### API Documentation

- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

## Testing

```bash
# Test with curl
curl -X POST http://localhost:8000/api/predict \
  -H "Content-Type: application/json" \
  -d '{"url": "https://google.com"}'

# Test phishing URL
curl -X POST http://localhost:8000/api/predict \
  -H "Content-Type: application/json" \
  -d '{"url": "http://paypal-secure-login.tk/verify"}'
```

## Performance

- Feature extraction: <50ms
- ML inference: <5ms
- Total latency: <100ms
- Throughput: 1000 req/s
- Accuracy: 95%+ (after retraining with real data)

## Docker

```bash
# Build image
docker build -t ml-service:latest .

# Run container
docker run -d -p 8000:8000 ml-service:latest
```
